{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Vanishing Gradient Problem & its Solution**\n\n# **The problem:**\n\nVanishing Gradient Problem (VGP) is a phenomenon that occurs during the training of deep neural networks. In this phenomenon, gradient descent is used to update the weights from an output layer to the ealier layer in back-propagation. During this process, the gradients become too small that they are almost vanished and do not further train the data.\n\n**Example:**\n\nConsider a very simple example on the which the principle of VGP is based. If a number smaller than 1 is multiplied multiple times, the resultant number will become too small. \n\n*                             0.1 X 0.1 X 0.1 X 0.1 = 0.0001 \n                 \n\n*                            W<sub>n</sub> = W<sub>old</sub> - η∂L/∂W\n\nThe change in ∂L/∂W would be so small that it would not change the weights in training the network. Therefore, the loss function after certain epochs whould not reduce, which will further result in worst accuracy of our model. \n\n# **Causes of VGP**\n\nThere are mainly two causes of the Vanishing Gradient Problem. They are:\n\n* Deep NeuraL Networks\n* Activation Functions (Sigmoid & tanh)\n\n# **How to recognize VGP?**\n\nBelow are the two main ways via which one can recognize Vanishing Gradient Problem (VGP). \n* **Focus on loss:** If there are no changes in loss after every epoch, then a problem of VGP exists. \n* **Plot weights:** Visualize weights against epochs. If the graph of weights is consistent and there is no change in weights, then it is a sign of VGP. \n\n# **How to handle VGP?**\n\n* **Reduce model complexity:** It means use shallow neural network, which in turn would give larger derivative of the weights in the initial layer. Reason being is that minimum number of derivatives will be multiplied in shallow model in comparison to the complex model.\n\n        This is not an efficient way because number of hidden layers is increased to find out complex patterns. It may work sometimes but will not be used mostly.\n        \n\n* **Use different activation function:** Use Relu activation function instead of sigmoid function. The vlaue of relu function is like this, max(0, z). If negative, the value will be 0. Otherwise, the value will be z. \n\n          The beauty of relu function is that it brings the inputs if they are in a range of (-1000, 1000) to (0, 1000). Whereas, the sigmoid function squeez the input to a range of (0, 1). \n          Another good thing is that the derivative will be either 0 or 1. If negative, it will be 0. Otherwise 1. When you multiply maximum numbers of 1, then then number does not reduced and VGP does not occur. \n        \n\n  ****When does the Relu function fail to counter VGP?**** \n  \n  The problem is **Dying Relu.** When the activation is 0, the derivatives will become 0, which inturn would not update weights. To solve dyink relu, the concept of leaky relu came into existence, which will be further explored in the upcoming notebooks.\n\n* **Other ways:**\n1. Proper Weight Initialization\n2. Batch Normalization\n3. Residual Network\n\n**Note:** The other ways will be explored in the upcoming notebooks. ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# **Importing Libraries**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense","metadata":{"execution":{"iopub.status.busy":"2023-09-26T09:10:55.281101Z","iopub.execute_input":"2023-09-26T09:10:55.281630Z","iopub.status.idle":"2023-09-26T09:11:00.604710Z","shell.execute_reply.started":"2023-09-26T09:10:55.281587Z","shell.execute_reply":"2023-09-26T09:11:00.602978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Loading make_moons dataset from Scikit-Learn**","metadata":{}},{"cell_type":"code","source":"X, y = make_moons(n_samples=250, noise=0.05, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T09:11:00.607670Z","iopub.execute_input":"2023-09-26T09:11:00.608677Z","iopub.status.idle":"2023-09-26T09:11:00.617170Z","shell.execute_reply.started":"2023-09-26T09:11:00.608628Z","shell.execute_reply":"2023-09-26T09:11:00.616086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(X[:, 0], X[:,1], c=y)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T09:11:00.618779Z","iopub.execute_input":"2023-09-26T09:11:00.619286Z","iopub.status.idle":"2023-09-26T09:11:01.010357Z","shell.execute_reply.started":"2023-09-26T09:11:00.619242Z","shell.execute_reply":"2023-09-26T09:11:01.008656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model Building**","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Dense(10, activation=\"relu\", input_dim=2))\nmodel.add(Dense(10, activation=\"relu\"))\nmodel.add(Dense(10, activation=\"relu\"))\nmodel.add(Dense(10, activation=\"relu\"))\nmodel.add(Dense(10, activation=\"relu\"))\nmodel.add(Dense(10, activation=\"relu\"))\nmodel.add(Dense(10, activation=\"relu\"))\nmodel.add(Dense(10, activation=\"relu\"))\nmodel.add(Dense(10, activation=\"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))","metadata":{"execution":{"iopub.status.busy":"2023-09-26T09:11:01.013947Z","iopub.execute_input":"2023-09-26T09:11:01.014403Z","iopub.status.idle":"2023-09-26T09:11:01.271656Z","shell.execute_reply.started":"2023-09-26T09:11:01.014367Z","shell.execute_reply":"2023-09-26T09:11:01.270122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Points to Ponder:**\n\n* To depict the phenomenon of Vanishing Gradient Problem (VGP), 10 hidden layers were used with different epochs such as 1 and 100. \n\n* To reduce model complexity and solve Vansishing Gradient Problem (VGP), only two hidden layers are used. \n\n* To solve VGP via activation function, sigmoid was changed to relu activation function and the number of hidden layers were increased to 10 again.  ","metadata":{}},{"cell_type":"markdown","source":"## **Model Compiling**","metadata":{}},{"cell_type":"code","source":"model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2023-09-26T09:11:01.275644Z","iopub.execute_input":"2023-09-26T09:11:01.276135Z","iopub.status.idle":"2023-09-26T09:11:01.521325Z","shell.execute_reply.started":"2023-09-26T09:11:01.276102Z","shell.execute_reply":"2023-09-26T09:11:01.519752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Old Weights**","metadata":{}},{"cell_type":"code","source":"old_weights = model.get_weights()[0]\nold_weights","metadata":{"execution":{"iopub.status.busy":"2023-09-26T09:11:01.522564Z","iopub.execute_input":"2023-09-26T09:11:01.522903Z","iopub.status.idle":"2023-09-26T09:11:01.545281Z","shell.execute_reply.started":"2023-09-26T09:11:01.522875Z","shell.execute_reply":"2023-09-26T09:11:01.543592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Data Splitting**","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T09:11:01.547046Z","iopub.execute_input":"2023-09-26T09:11:01.547429Z","iopub.status.idle":"2023-09-26T09:11:01.556384Z","shell.execute_reply.started":"2023-09-26T09:11:01.547398Z","shell.execute_reply":"2023-09-26T09:11:01.554567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Model Fitting**","metadata":{}},{"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=100)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T09:11:01.558344Z","iopub.execute_input":"2023-09-26T09:11:01.558837Z","iopub.status.idle":"2023-09-26T09:11:06.935190Z","shell.execute_reply.started":"2023-09-26T09:11:01.558794Z","shell.execute_reply":"2023-09-26T09:11:06.933655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **New Weights**","metadata":{}},{"cell_type":"code","source":"new_weights = model.get_weights()[0]\nnew_weights","metadata":{"execution":{"iopub.status.busy":"2023-09-26T09:11:06.937490Z","iopub.execute_input":"2023-09-26T09:11:06.937993Z","iopub.status.idle":"2023-09-26T09:11:06.955003Z","shell.execute_reply.started":"2023-09-26T09:11:06.937948Z","shell.execute_reply":"2023-09-26T09:11:06.953206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.optimizer.get_config()[\"learning_rate\"]","metadata":{"execution":{"iopub.status.busy":"2023-09-26T09:11:06.959190Z","iopub.execute_input":"2023-09-26T09:11:06.959783Z","iopub.status.idle":"2023-09-26T09:11:06.971686Z","shell.execute_reply.started":"2023-09-26T09:11:06.959739Z","shell.execute_reply":"2023-09-26T09:11:06.970194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We know that \n\n\n*                            W<sub>n</sub> = W<sub>old</sub> - η * ∂L/∂W\n\nThe value of ∂L/∂W will be\n\n*                            ∂L/∂W = (W<sub>old</sub> - W<sub>n</sub>) / η\n","metadata":{}},{"cell_type":"markdown","source":"# **Analysis of Weights and Gradients**","metadata":{}},{"cell_type":"code","source":"print(f\"Old weights are: \\n\\n {old_weights} \\n\")\nprint(f\"New weights are: \\n\\n {new_weights} \\n\")\ngradient = (old_weights - new_weights) / 0.001\npercent_chng = abs(100*(old_weights - new_weights) / old_weights)\n\nprint(f\"Gradients are: \\n\\n {gradient} \\n\")\nprint(f\"Percent changes are: \\n\\n {percent_chng} \\n\")","metadata":{"execution":{"iopub.status.busy":"2023-09-26T09:11:06.973444Z","iopub.execute_input":"2023-09-26T09:11:06.973996Z","iopub.status.idle":"2023-09-26T09:11:06.989649Z","shell.execute_reply.started":"2023-09-26T09:11:06.973949Z","shell.execute_reply":"2023-09-26T09:11:06.987978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Conclusions**\n\nRuning it for different values of epochs and hidden layers, following conclusions have been made. \n\n* When the value of epoch was 1, the model gave us small values of gradients and weights did not get update. \n\n* When the value of epoch was 100, the model stopped training further after almost 15 epochs. \n\n* When the number of hidden layers were reduced to 3, the loss function reduced too much. It stop fluctauting around a single point. \n\n* With changing model complexity to shallow neural network, the problem of Vanishing Gradient Problem solved. \n\n* With changing relu function as an activation function, the problem of Vanishing Problem solved. \n\n\n**Point to Ponder:**\n\n1. Reducing hidden layers in a model is not an effiecient way as compared to changing activation function from sigmoid to relu.\n\n2. The percent changes in old weights and new weights are comparatively higher than when it was run to show depiction of Vaninshing Gradient Problem. ","metadata":{}}]}